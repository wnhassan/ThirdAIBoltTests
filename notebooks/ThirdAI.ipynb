{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thirdai\n",
    "thirdai.licensing.activate(\"D2A996-E7598D-DB122B-E29544-951D7A-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contract Review', 'Finance QnA', 'General QnA']\n"
     ]
    }
   ],
   "source": [
    "from thirdai import neural_db, licensing\n",
    "\n",
    "#licensing.activate(\"YOUR LICENSE KEY HERE\")\n",
    "\n",
    "ndb = neural_db.NeuralDB()\n",
    "#Additionally, we provide a suite of pre-trained databases from our Model Bazaar. To download a pre-trained model:\n",
    "from thirdai.neural_db import Bazaar\n",
    "# Set up a cache directory\n",
    "import os\n",
    "if not os.path.isdir(\"bazaar_cache\"):\n",
    "    os.mkdir(\"bazaar_cache\")\n",
    "from pathlib import Path\n",
    "\n",
    "bazaar = Bazaar(cache_dir=Path(\"bazaar_cache\"))\n",
    "bazaar.fetch()\n",
    "\n",
    "# This will return all the available pre-trained models\n",
    "print(bazaar.list_model_names())\n",
    "\n",
    "# pass the string identifier of the model you'd like to use. General QnA is our \n",
    "# most generic and foundational pre-trained model for the majority of use cases. \n",
    "ndb = bazaar.get_model(\"General QnA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'thirdai.neural_db.neural_db.NeuralDB'>\n",
      "Help on class PDF in module thirdai.neural_db.documents:\n",
      "\n",
      "class PDF(Extracted)\n",
      " |  PDF(path: str)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PDF\n",
      " |      Extracted\n",
      " |      Document\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path: str)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  process_data(self, path: str) -> pandas.core.frame.DataFrame\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Extracted:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  context(self, element_id, radius) -> str\n",
      " |  \n",
      " |  load_meta(self, directory: pathlib.Path)\n",
      " |  \n",
      " |  reference(self, element_id: int) -> thirdai.neural_db.documents.Reference\n",
      " |  \n",
      " |  save_meta(self, directory: pathlib.Path)\n",
      " |  \n",
      " |  show_fn(text, source, **kwargs)\n",
      " |  \n",
      " |  strong_text(self, element_id: int) -> str\n",
      " |  \n",
      " |  weak_text(self, element_id: int) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from Extracted:\n",
      " |  \n",
      " |  hash\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Document:\n",
      " |  \n",
      " |  save(self, directory: str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from Document:\n",
      " |  \n",
      " |  load(directory: str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Document:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  save_extra_info\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'context',\n",
       " 'hash',\n",
       " 'load',\n",
       " 'load_meta',\n",
       " 'name',\n",
       " 'process_data',\n",
       " 'reference',\n",
       " 'save',\n",
       " 'save_extra_info',\n",
       " 'save_meta',\n",
       " 'show_fn',\n",
       " 'size',\n",
       " 'strong_text',\n",
       " 'weak_text']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(ndb))\n",
    "help(neural_db.PDF)\n",
    "dir(neural_db.PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/2009_barack_obama_d.pdf', '../data/2010_barack_obama_d.pdf', '../data/2011_barack_obama_d.pdf', '../data/2012_barack_obama_d.pdf', '../data/2013_barack_obama_d.pdf', '../data/2014_barack_obama_d.pdf', '../data/2015_barack_obama_d.pdf', '../data/2016_barack_obama_d.pdf', '../data/2017_donald_j_trump_r.pdf', '../data/2018_donald_j_trump_r.pdf', '../data/2019_donald_j_trump_r.pdf', '../data/2020_donald_j_trump_r.pdf', '../data/2021_joseph_r_biden_d.pdf', '../data/sample_nda.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = '../data/'\n",
    "pdf_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.pdf')]\n",
    "\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below code is to generate a sample csv file which takes a certain random sentences from the file and puts that into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "\n",
    "def extract_sentences_from_pdf(file_path, rnd_num_sentences=1):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "        \n",
    "        # Split the text into sentences based on full stops\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        # Randomly select 'rnd_num_sentences' sentences\n",
    "        if len(sentences) >= rnd_num_sentences:\n",
    "            selected_sentences = random.sample(sentences, rnd_num_sentences)\n",
    "        else:\n",
    "            selected_sentences = sentences\n",
    "        \n",
    "        return selected_sentences\n",
    "\n",
    "#pdf_files = ['../data/2009_barack_obama_d.pdf', '../data/2010_barack_obama_d.pdf', '../data/2021_joseph_r_biden_d.pdf', '../data/sample_nda.pdf']\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    sentences = extract_sentences_from_pdf(file_path)\n",
    "    for sentence in sentences:\n",
    "        # Extract the last part of the sentence (around 10-15 words)\n",
    "        words = sentence.split()\n",
    "        if len(words) > 15:\n",
    "            sentence = ' '.join(words[-15:])\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        all_data.append((file_name, sentence))\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=['File Name', 'Sentence'])\n",
    "df.to_csv(\"random_sentences_output.csv\", index=False)\n",
    "df  # This will display the DataFrame in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the code the guys gave does not work as is...at least for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralDB' object has no attribute 'PDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\ThirdAI.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#pdf_files = ['../data/sample_nda.pdf']\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m pdf_files:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pdf_doc \u001b[39m=\u001b[39m ndb\u001b[39m.\u001b[39;49mPDF(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     insertable_docs\u001b[39m.\u001b[39mappend(pdf_files)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# my_dict = {str(i): item for i, item in enumerate(insertable_docs)}\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Result: {0: 'a', 1: 'b', 2: 'c'}\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralDB' object has no attribute 'PDF'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "insertable_docs = []\n",
    "#pdf_files = ['../data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = ndb.PDF(file)\n",
    "    \n",
    "    insertable_docs.append(pdf_files)\n",
    "    \n",
    "    # my_dict = {str(i): item for i, item in enumerate(insertable_docs)}\n",
    "    # Result: {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "source_ids = ndb.insert(insertable_docs, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wnhas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting File: ../data/2009_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2010_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2011_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2012_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2013_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2014_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2015_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2016_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2017_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2018_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2019_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2020_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2021_joseph_r_biden_d.pdf\n",
      "\n",
      "Inserting File: ../data/sample_nda.pdf\n"
     ]
    }
   ],
   "source": [
    "from thirdai.neural_db.documents import PDF\n",
    "from thirdai.neural_db.neural_db import NeuralDB\n",
    "\n",
    "# Create a NeuralDB instance\n",
    "ndb = NeuralDB()\n",
    "\n",
    "insertable_docs = []\n",
    "#pdf_files = ['../data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = PDF(file)  # Using PDF class directly\n",
    "    print (\"\\nInserting File:\",file)\n",
    "    insertable_docs.append(pdf_doc)  # You should append the pdf_doc object, not pdf_files\n",
    "\n",
    "source_ids = ndb.insert(insertable_docs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below code is for searching. will modify it shortly to take the random sample sentences from the files and see if ThirdAI can pick them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = ndb.search(\n",
    "    query=\"military benefits\",\n",
    "    top_k=4,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentences from the CSV\n",
    "df = pd.read_csv(\"random_sentences_output.csv\")\n",
    "\n",
    "# Columns to store the found sentences and their file names\n",
    "\n",
    "# df[\"Found Sentence\"] = None\n",
    "# df[\"Found In File\"] = None\n",
    "\n",
    "########################################################################\n",
    "# Function to query the neural DB and store results SINGLE RESULT.... WILL BE REMOVED\n",
    "# def query_neural_db(sentence):\n",
    "#     search_results = ndb.search(\n",
    "#         query=sentence,\n",
    "#         top_k=4,\n",
    "#         on_error=lambda error_msg: print(f\"Error! {error_msg}\")\n",
    "#     )\n",
    "    \n",
    "#     # the search results contain a sentence and its corresponding file name.\n",
    "#     found_sentence = search_results[0].text if search_results else None\n",
    "#     found_file = search_results[0].source if search_results else None\n",
    "\n",
    "#     return found_sentence, found_file\n",
    "###########################################################################\n",
    "def query_neural_db(sentence):\n",
    "    search_results = ndb.search(\n",
    "        query=sentence,\n",
    "        top_k=4,\n",
    "        on_error=lambda error_msg: print(f\"Error! {error_msg}\")\n",
    "    )\n",
    "    \n",
    "    found_sentences = []\n",
    "    found_files = []\n",
    "    \n",
    "    for result in search_results:\n",
    "        found_sentences.append(result.text)\n",
    "        found_files.append(result.source)\n",
    "\n",
    "    return found_sentences, found_files\n",
    "\n",
    "\n",
    "# Query the neural DB for each sentence\n",
    "for idx, row in df.iterrows():\n",
    "    search_string = row[\"Sentence\"]\n",
    "    found_sentence, found_file = query_neural_db(search_string)\n",
    "    #save the first 3 top results from the returned data from NeuralDB\n",
    "    df.at[idx, \"Found Sentence_1\"] = found_sentence[0]\n",
    "    df.at[idx, \"Found In File_1\"] = os.path.basename(found_file[0])\n",
    "    df.at[idx, \"Found Sentence_2\"] = found_sentence[1]\n",
    "    df.at[idx, \"Found In File_2\"] = os.path.basename(found_file[1])\n",
    "    df.at[idx, \"Found Sentence_3\"] = found_sentence[2]\n",
    "    df.at[idx, \"Found In File_3\"] = os.path.basename(found_file[2])\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"sentences_search.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:504: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample_state_of_union.ndb'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save your db\n",
    "ndb.save(\"sample_state_of_union.ndb\")\n",
    "\n",
    "# <!-- # Loading is just like we showed above, with an optional progress handler\n",
    "# db.from_checkpoint(\"sample_nda.ndb\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\")) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.666666666666664% done with loading.\n",
      "33.33333333333333% done with loading.\n",
      "50.0% done with loading.\n",
      "66.66666666666666% done with loading.\n",
      "83.33333333333334% done with loading.\n",
      "100.0% done with loading.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<thirdai.neural_db.neural_db.NeuralDB at 0x1e708d7c610>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndb.from_checkpoint(\"sample_state_of_union.ndb\", on_progress=lambda fraction: print(f\"{fraction*100}% done with loading.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will ensure equal treatment for all servicemembers and equal benefits for their families gay and straight. We will draw upon the courage and skills of our sisters and daughters and moms because women have proven under fire that they are ready for combat. We will keep faith with our veterans investing in world-class care-including mental health care-for our wounded warriors supporting our military families giving our veterans the benefits and education and job opportunities that they have earned.\n",
      "c:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "Tonight let us speak with one voice in reaffirming that our Nation is united in support of our troops and their families. Let us serve them as well as they've served us by giving them the equipment they need by providing them with the care and benefits that they have earned and by enlisting our veterans in the great task of building our own Nation.\n",
      "c:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\..\\data\\2011_barack_obama_d.pdf\n",
      "************\n",
      "I am thrilled to report to you tonight that our economy is the best it has ever been. Our military is completely rebuilt with its power being unmatched anywhere in the world -- and it is not even close. Our borders are secure. Our families are flourishing.\n",
      "c:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\..\\data\\2020_donald_j_trump_r.pdf\n",
      "************\n",
      "We'll keep file:///C/WalidLenovo/OneDrive/Walid/_2023/USB/LLMs/FilesToSearch/2014_barack_obama_d.txt[9/29/2023 4:38:37 PM] working to help all our veterans translate their skills and leadership into jobs here at home. And we will all continue to join forces to honor and support our remarkable military families. Let me tell you about one of those families I've come to know.\n",
      "c:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\..\\data\\2014_barack_obama_d.pdf\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = ndb.search(\n",
    "    query=\"military benefits\",\n",
    "    top_k=4,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
