{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thirdai\n",
    "thirdai.licensing.activate(\"D2A996-E7598D-DB122B-E29544-951D7A-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contract Review', 'Finance QnA', 'General QnA']\n"
     ]
    }
   ],
   "source": [
    "from thirdai import neural_db, licensing\n",
    "\n",
    "#licensing.activate(\"YOUR LICENSE KEY HERE\")\n",
    "\n",
    "ndb = neural_db.NeuralDB()\n",
    "#Additionally, we provide a suite of pre-trained databases from our Model Bazaar. To download a pre-trained model:\n",
    "from thirdai.neural_db import Bazaar\n",
    "# Set up a cache directory\n",
    "import os\n",
    "if not os.path.isdir(\"bazaar_cache\"):\n",
    "    os.mkdir(\"bazaar_cache\")\n",
    "from pathlib import Path\n",
    "\n",
    "bazaar = Bazaar(cache_dir=Path(\"../stored_dbs/bazaar_cache\"))\n",
    "bazaar.fetch()\n",
    "\n",
    "# This will return all the available pre-trained models\n",
    "print(bazaar.list_model_names())\n",
    "\n",
    "# pass the string identifier of the model you'd like to use. General QnA is our \n",
    "# most generic and foundational pre-trained model for the majority of use cases. \n",
    "ndb = bazaar.get_model(\"General QnA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'thirdai.neural_db.neural_db.NeuralDB'>\n",
      "Help on class PDF in module thirdai.neural_db.documents:\n",
      "\n",
      "class PDF(Extracted)\n",
      " |  PDF(path: str, metadata={})\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PDF\n",
      " |      Extracted\n",
      " |      Document\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path: str, metadata={})\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  process_data(self, path: str) -> pandas.core.frame.DataFrame\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Extracted:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  all_entity_ids(self) -> List[int]\n",
      " |  \n",
      " |  context(self, element_id, radius) -> str\n",
      " |  \n",
      " |  load_meta(self, directory: pathlib.Path)\n",
      " |  \n",
      " |  reference(self, element_id: int) -> thirdai.neural_db.documents.Reference\n",
      " |  \n",
      " |  save_meta(self, directory: pathlib.Path)\n",
      " |  \n",
      " |  show_fn(text, source, **kwargs)\n",
      " |  \n",
      " |  strong_text(self, element_id: int) -> str\n",
      " |  \n",
      " |  weak_text(self, element_id: int) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from Extracted:\n",
      " |  \n",
      " |  hash\n",
      " |  \n",
      " |  matched_constraints\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Document:\n",
      " |  \n",
      " |  save(self, directory: str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from Document:\n",
      " |  \n",
      " |  load(directory: str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Document:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  save_extra_info\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'all_entity_ids',\n",
       " 'context',\n",
       " 'hash',\n",
       " 'load',\n",
       " 'load_meta',\n",
       " 'matched_constraints',\n",
       " 'name',\n",
       " 'process_data',\n",
       " 'reference',\n",
       " 'save',\n",
       " 'save_extra_info',\n",
       " 'save_meta',\n",
       " 'show_fn',\n",
       " 'size',\n",
       " 'strong_text',\n",
       " 'weak_text']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(ndb))\n",
    "help(neural_db.PDF)\n",
    "dir(neural_db.PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/2009_barack_obama_d.pdf', '../data/2010_barack_obama_d.pdf', '../data/2011_barack_obama_d.pdf', '../data/2012_barack_obama_d.pdf', '../data/2013_barack_obama_d.pdf', '../data/2014_barack_obama_d.pdf', '../data/2015_barack_obama_d.pdf', '../data/2016_barack_obama_d.pdf', '../data/2017_donald_j_trump_r.pdf', '../data/2018_donald_j_trump_r.pdf', '../data/2019_donald_j_trump_r.pdf', '../data/2020_donald_j_trump_r.pdf', '../data/2021_joseph_r_biden_d.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = '../data/'\n",
    "pdf_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.pdf')]\n",
    "\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below code is to generate a sample csv file which takes a certain random sentences from the file and puts that into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009_barack_obama_d.pdf</td>\n",
       "      <td>we failed to look beyond the next payment, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010_barack_obama_d.pdf</td>\n",
       "      <td>But we still need to govern.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011_barack_obama_d.pdf</td>\n",
       "      <td>As Robert Kennedy told us:  file:///C/WalidLen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012_barack_obama_d.pdf</td>\n",
       "      <td>We bet on American ingenuity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013_barack_obama_d.pdf</td>\n",
       "      <td>creates good, middle class jobs, that must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014_barack_obama_d.pdf</td>\n",
       "      <td>marches the red, white, and blue into the Olym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015_barack_obama_d.pdf</td>\n",
       "      <td>That's what makes us exceptional.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016_barack_obama_d.pdf</td>\n",
       "      <td>correctness, this is a matter of understanding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017_donald_j_trump_r.pdf</td>\n",
       "      <td>I am.I believe strongly in free trade, but it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018_donald_j_trump_r.pdf</td>\n",
       "      <td>The Holets named their new daughter Hope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019_donald_j_trump_r.pdf</td>\n",
       "      <td>and foe alike must never doubt this Nation's p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020_donald_j_trump_r.pdf</td>\n",
       "      <td>million new jobs — 5 million more than Governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021_joseph_r_biden_d.pdf</td>\n",
       "      <td>Extraordinary courage was summoned.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    File Name  \\\n",
       "0     2009_barack_obama_d.pdf   \n",
       "1     2010_barack_obama_d.pdf   \n",
       "2     2011_barack_obama_d.pdf   \n",
       "3     2012_barack_obama_d.pdf   \n",
       "4     2013_barack_obama_d.pdf   \n",
       "5     2014_barack_obama_d.pdf   \n",
       "6     2015_barack_obama_d.pdf   \n",
       "7     2016_barack_obama_d.pdf   \n",
       "8   2017_donald_j_trump_r.pdf   \n",
       "9   2018_donald_j_trump_r.pdf   \n",
       "10  2019_donald_j_trump_r.pdf   \n",
       "11  2020_donald_j_trump_r.pdf   \n",
       "12  2021_joseph_r_biden_d.pdf   \n",
       "\n",
       "                                             Sentence  \n",
       "0   we failed to look beyond the next payment, the...  \n",
       "1                        But we still need to govern.  \n",
       "2   As Robert Kennedy told us:  file:///C/WalidLen...  \n",
       "3                       We bet on American ingenuity.  \n",
       "4   creates good, middle class jobs, that must be ...  \n",
       "5   marches the red, white, and blue into the Olym...  \n",
       "6                   That's what makes us exceptional.  \n",
       "7   correctness, this is a matter of understanding...  \n",
       "8   I am.I believe strongly in free trade, but it ...  \n",
       "9           The Holets named their new daughter Hope.  \n",
       "10  and foe alike must never doubt this Nation's p...  \n",
       "11  million new jobs — 5 million more than Governm...  \n",
       "12                Extraordinary courage was summoned.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "\n",
    "def extract_sentences_from_pdf(file_path, rnd_num_sentences=1):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "        \n",
    "        # Split the text into sentences based on full stops\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        # Randomly select 'rnd_num_sentences' sentences\n",
    "        if len(sentences) >= rnd_num_sentences:\n",
    "            selected_sentences = random.sample(sentences, rnd_num_sentences)\n",
    "        else:\n",
    "            selected_sentences = sentences\n",
    "        \n",
    "        return selected_sentences\n",
    "\n",
    "#pdf_files = ['../data/2009_barack_obama_d.pdf', '../data/2010_barack_obama_d.pdf', '../data/2021_joseph_r_biden_d.pdf', '../data/sample_nda.pdf']\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    sentences = extract_sentences_from_pdf(file_path)\n",
    "    for sentence in sentences:\n",
    "        # Extract the last part of the sentence (around 10-15 words)\n",
    "        words = sentence.split()\n",
    "        if len(words) > 15:\n",
    "            sentence = ' '.join(words[-15:])\n",
    "            \n",
    "        file_name = os.path.basename(file_path)\n",
    "        all_data.append((file_name, sentence))\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=['File Name', 'Sentence'])\n",
    "df.to_csv(\"random_sentences_output.csv\", index=False)\n",
    "df  # This will display the DataFrame in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the code the guys gave does not work as is...at least for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralDB' object has no attribute 'PDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\ThirdAI.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#pdf_files = ['../data/sample_nda.pdf']\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m pdf_files:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pdf_doc \u001b[39m=\u001b[39m ndb\u001b[39m.\u001b[39;49mPDF(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     insertable_docs\u001b[39m.\u001b[39mappend(pdf_files)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# my_dict = {str(i): item for i, item in enumerate(insertable_docs)}\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Result: {0: 'a', 1: 'b', 2: 'c'}\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralDB' object has no attribute 'PDF'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "insertable_docs = []\n",
    "#pdf_files = ['../data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = ndb.PDF(file)\n",
    "    \n",
    "    insertable_docs.append(pdf_files)\n",
    "    \n",
    "    # my_dict = {str(i): item for i, item in enumerate(insertable_docs)}\n",
    "    # Result: {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "source_ids = ndb.insert(insertable_docs, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wnhas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting File: ../data/2009_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2010_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2011_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2012_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2013_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2014_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2015_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2016_barack_obama_d.pdf\n",
      "\n",
      "Inserting File: ../data/2017_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2018_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2019_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2020_donald_j_trump_r.pdf\n",
      "\n",
      "Inserting File: ../data/2021_joseph_r_biden_d.pdf\n"
     ]
    }
   ],
   "source": [
    "from thirdai.neural_db.documents import PDF\n",
    "from thirdai.neural_db.neural_db import NeuralDB\n",
    "\n",
    "# Create a NeuralDB instance\n",
    "ndb = NeuralDB()\n",
    "\n",
    "insertable_docs = []\n",
    "#pdf_files = ['../data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = PDF(file)  # Using PDF class directly\n",
    "    print (\"\\nInserting File:\",file)\n",
    "    insertable_docs.append(pdf_doc)  # You should append the pdf_doc object, not pdf_files\n",
    "\n",
    "source_ids = ndb.insert(insertable_docs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below code is for searching. will modify it shortly to take the random sample sentences from the files and see if ThirdAI can pick them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will ensure equal treatment for all servicemembers and equal benefits for their families gay and straight. We will draw upon the courage and skills of our sisters and daughters and moms because women have proven under fire that they are ready for combat. We will keep faith with our veterans investing in world-class care-including mental health care-for our wounded warriors supporting our military families giving our veterans the benefits and education and job opportunities that they have earned.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "Can you blame them for feeling a little cynical? The greatest blow to our confidence in our economy last year didn't come from events beyond our control. It came from a debate in Washington over whether the United States would pay its bills or not. Who benefited from that fiasco?\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2012_barack_obama_d.pdf\n",
      "************\n",
      "So in the months ahead I will continue to engage Congress to ensure not only that our targeting detention and prosecution of terrorists remains consistent with our laws and system of checks and balances but that our efforts are even more transparent to the American people and to the world.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "It's a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet in America we do not enforce this rule straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences our current immigration system costs American taxpayers many billions of dollars a year.Switching away from this current system of lower skilled immigration and instead adopting a merit-based system we will have so many more benefits.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2017_donald_j_trump_r.pdf\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = ndb.search(\n",
    "    query=\"military benefits\",\n",
    "    top_k=4,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:48: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'In other words we have lived through an era where too often short-term gains were prized over long-term prosperity where we failed to look beyond the next payment the next quarter or the next election. A surplus became an excuse to transfer wealth to the wealthy instead of an opportunity to invest in our future.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found Sentence_1\"] = found_sentence[0]\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2009_barack_obama_d.pdf' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found In File_1\"] = os.path.basename(found_file[0])\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'They're still the right thing to do. And I won't let up until they get done. But for my final address to this Chamber I don't want to just talk about next year. I want to focus on the next 5 years the next 10 years and beyond.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found Sentence_2\"] = found_sentence[1]\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:51: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2016_barack_obama_d.pdf' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found In File_2\"] = os.path.basename(found_file[1])\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'The regime is more isolated than ever before. Its leaders are faced with crippling sanctions and as long as they shirk their responsibilities this pressure will not relent. Let there be no doubt: America is determined to prevent Iran from getting a nuclear weapon and I will take no options off the table to achieve that goal.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found Sentence_3\"] = found_sentence[2]\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:53: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2012_barack_obama_d.pdf' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found In File_3\"] = os.path.basename(found_file[2])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentences from the CSV\n",
    "df = pd.read_csv(\"random_sentences_output.csv\")\n",
    "\n",
    "# Columns to store the found sentences and their file names\n",
    "\n",
    "# df[\"Found Sentence\"] = None\n",
    "# df[\"Found In File\"] = None\n",
    "\n",
    "########################################################################\n",
    "# Function to query the neural DB and store results SINGLE RESULT.... WILL BE REMOVED\n",
    "# def query_neural_db(sentence):\n",
    "#     search_results = ndb.search(\n",
    "#         query=sentence,\n",
    "#         top_k=4,\n",
    "#         on_error=lambda error_msg: print(f\"Error! {error_msg}\")\n",
    "#     )\n",
    "    \n",
    "#     # the search results contain a sentence and its corresponding file name.\n",
    "#     found_sentence = search_results[0].text if search_results else None\n",
    "#     found_file = search_results[0].source if search_results else None\n",
    "\n",
    "#     return found_sentence, found_file\n",
    "###########################################################################\n",
    "def query_neural_db(sentence):\n",
    "    search_results = ndb.search(\n",
    "        query=sentence,\n",
    "        top_k=4,\n",
    "        on_error=lambda error_msg: print(f\"Error! {error_msg}\")\n",
    "    )\n",
    "    \n",
    "    found_sentences = []\n",
    "    found_files = []\n",
    "    \n",
    "    for result in search_results:\n",
    "        found_sentences.append(result.text)\n",
    "        found_files.append(result.source)\n",
    "\n",
    "    return found_sentences, found_files\n",
    "\n",
    "\n",
    "# Query the neural DB for each sentence\n",
    "for idx, row in df.iterrows():\n",
    "    search_string = row[\"Sentence\"]\n",
    "    found_sentence, found_file = query_neural_db(search_string)\n",
    "    #save the first 3 top results from the returned data from NeuralDB\n",
    "    df.at[idx, \"Found Sentence_1\"] = found_sentence[0]\n",
    "    df.at[idx, \"Found In File_1\"] = os.path.basename(found_file[0])\n",
    "    df.at[idx, \"Found Sentence_2\"] = found_sentence[1]\n",
    "    df.at[idx, \"Found In File_2\"] = os.path.basename(found_file[1])\n",
    "    df.at[idx, \"Found Sentence_3\"] = found_sentence[2]\n",
    "    df.at[idx, \"Found In File_3\"] = os.path.basename(found_file[2])\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"sentences_search.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\stored_dbs\\\\sample_state_of_union.ndb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save your db\n",
    "ndb.save(\"../stored_dbs/sample_state_of_union.ndb\")\n",
    "\n",
    "# <!-- # Loading is just like we showed above, with an optional progress handler\n",
    "# db.from_checkpoint(\"sample_nda.ndb\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\")) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.666666666666664% done with loading.\n",
      "33.33333333333333% done with loading.\n",
      "50.0% done with loading.\n",
      "66.66666666666666% done with loading.\n",
      "83.33333333333334% done with loading.\n",
      "100.0% done with loading.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<thirdai.neural_db.neural_db.NeuralDB at 0x214041c7cd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndb.from_checkpoint(\"../stored_dbs/sample_state_of_union.ndb\", on_progress=lambda fraction: print(f\"{fraction*100}% done with loading.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below is a retrieval of the stored data base and search for the same sentence\n",
    "#### intresting enough, not the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will ensure equal treatment for all servicemembers and equal benefits for their families gay and straight. We will draw upon the courage and skills of our sisters and daughters and moms because women have proven under fire that they are ready for combat. We will keep faith with our veterans investing in world-class care-including mental health care-for our wounded warriors supporting our military families giving our veterans the benefits and education and job opportunities that they have earned.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "Can you blame them for feeling a little cynical? The greatest blow to our confidence in our economy last year didn't come from events beyond our control. It came from a debate in Washington over whether the United States would pay its bills or not. Who benefited from that fiasco?\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2012_barack_obama_d.pdf\n",
      "************\n",
      "So in the months ahead I will continue to engage Congress to ensure not only that our targeting detention and prosecution of terrorists remains consistent with our laws and system of checks and balances but that our efforts are even more transparent to the American people and to the world.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "It's a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet in America we do not enforce this rule straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences our current immigration system costs American taxpayers many billions of dollars a year.Switching away from this current system of lower skilled immigration and instead adopting a merit-based system we will have so many more benefits.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2017_donald_j_trump_r.pdf\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = ndb.search(\n",
    "    query=\"military benefits\",\n",
    "    top_k=4,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
