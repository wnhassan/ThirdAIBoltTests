{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thirdai\n",
    "\n",
    "thirdai.licensing.activate(\"D2A996-E7598D-DB122B-E29544-951D7A-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contract Review', 'Finance QnA', 'General QnA']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from thirdai.neural_db import Bazaar\n",
    "from thirdai import neural_db, licensing\n",
    "\n",
    "# licensing.activate(\"YOUR LICENSE KEY HERE\")\n",
    "\n",
    "ndb = neural_db.NeuralDB()\n",
    "# Additionally, we provide a suite of pre-trained databases from our Model Bazaar. To download a pre-trained model:\n",
    "\n",
    "# Set up a cache directory\n",
    "\n",
    "if not os.path.isdir(\"bazaar_cache\"):\n",
    "    os.mkdir(\"bazaar_cache\")\n",
    "\n",
    "bazaar = Bazaar(cache_dir=Path(\"../stored_dbs/bazaar_cache\"))\n",
    "bazaar.fetch()\n",
    "\n",
    "# This will return all the available pre-trained models\n",
    "print(bazaar.list_model_names())\n",
    "\n",
    "# pass the string identifier of the model you'd like to use. General QnA is our\n",
    "# most generic and foundational pre-trained model for the majority of use cases.\n",
    "ndb = bazaar.get_model(\"General QnA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ndb))\n",
    "help(neural_db.PDF)\n",
    "dir(neural_db.PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/2009_barack_obama_d.pdf', '../data/2010_barack_obama_d.pdf', '../data/2011_barack_obama_d.pdf', '../data/2012_barack_obama_d.pdf', '../data/2013_barack_obama_d.pdf', '../data/2014_barack_obama_d.pdf', '../data/2015_barack_obama_d.pdf', '../data/2016_barack_obama_d.pdf', '../data/2017_donald_j_trump_r.pdf', '../data/2018_donald_j_trump_r.pdf', '../data/2019_donald_j_trump_r.pdf', '../data/2020_donald_j_trump_r.pdf', '../data/2021_joseph_r_biden_d.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DIRECTORY_PATH = \"../data/\"\n",
    "pdf_files = [\n",
    "    os.path.join(DIRECTORY_PATH, file)\n",
    "    for file in os.listdir(DIRECTORY_PATH)\n",
    "    if file.endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below code is to generate a sample csv file which takes a certain random sentences from the file and puts that into a CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009_barack_obama_d.pdf</td>\n",
       "      <td>Tax Code by finally ending the tax breaks for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009_barack_obama_d.pdf</td>\n",
       "      <td>money to pad their paychecks or buy fancy drap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010_barack_obama_d.pdf</td>\n",
       "      <td>me his allowance and asked if I would give it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010_barack_obama_d.pdf</td>\n",
       "      <td>That's what helped us into this crisis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011_barack_obama_d.pdf</td>\n",
       "      <td>you want to make a difference in the life of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011_barack_obama_d.pdf</td>\n",
       "      <td>into a system that's not working, we launched ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012_barack_obama_d.pdf</td>\n",
       "      <td>this Congress needs to stop the interest rates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012_barack_obama_d.pdf</td>\n",
       "      <td>have earned, which is why we've increased annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013_barack_obama_d.pdf</td>\n",
       "      <td>This single step would raise the incomes of mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013_barack_obama_d.pdf</td>\n",
       "      <td>But we can't stop there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014_barack_obama_d.pdf</td>\n",
       "      <td>A few months later, on his 10th deployment, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014_barack_obama_d.pdf</td>\n",
       "      <td>have declared that China is no longer the worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015_barack_obama_d.pdf</td>\n",
       "      <td>every day, live the idea that we are our broth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015_barack_obama_d.pdf</td>\n",
       "      <td>Every three weeks, we bring online as much sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016_barack_obama_d.pdf</td>\n",
       "      <td>We do them no favor when we don't show  \\nthem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016_barack_obama_d.pdf</td>\n",
       "      <td>But most of all, it contradicts everything tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017_donald_j_trump_r.pdf</td>\n",
       "      <td>It's been a long time since we had fair trade.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017_donald_j_trump_r.pdf</td>\n",
       "      <td>We are  \\none people with one destiny.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018_donald_j_trump_r.pdf</td>\n",
       "      <td>And I asked CJ, \"What's the secret?\"  \\nHe sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018_donald_j_trump_r.pdf</td>\n",
       "      <td>And it's the people who  \\nare making America ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019_donald_j_trump_r.pdf</td>\n",
       "      <td>to prohibit the late-term abortion of children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019_donald_j_trump_r.pdf</td>\n",
       "      <td>In 1997, Alice was sentenced to life in prison...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020_donald_j_trump_r.pdf</td>\n",
       "      <td>The days of our country being used, taken adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020_donald_j_trump_r.pdf</td>\n",
       "      <td>because no parent should be forced to send the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021_joseph_r_biden_d.pdf</td>\n",
       "      <td>file:///C/WalidLenovo/SEAS6800Local/USBankLLM/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021_joseph_r_biden_d.pdf</td>\n",
       "      <td>history has been one of the greatest logistica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    File Name  \\\n",
       "0     2009_barack_obama_d.pdf   \n",
       "1     2009_barack_obama_d.pdf   \n",
       "2     2010_barack_obama_d.pdf   \n",
       "3     2010_barack_obama_d.pdf   \n",
       "4     2011_barack_obama_d.pdf   \n",
       "5     2011_barack_obama_d.pdf   \n",
       "6     2012_barack_obama_d.pdf   \n",
       "7     2012_barack_obama_d.pdf   \n",
       "8     2013_barack_obama_d.pdf   \n",
       "9     2013_barack_obama_d.pdf   \n",
       "10    2014_barack_obama_d.pdf   \n",
       "11    2014_barack_obama_d.pdf   \n",
       "12    2015_barack_obama_d.pdf   \n",
       "13    2015_barack_obama_d.pdf   \n",
       "14    2016_barack_obama_d.pdf   \n",
       "15    2016_barack_obama_d.pdf   \n",
       "16  2017_donald_j_trump_r.pdf   \n",
       "17  2017_donald_j_trump_r.pdf   \n",
       "18  2018_donald_j_trump_r.pdf   \n",
       "19  2018_donald_j_trump_r.pdf   \n",
       "20  2019_donald_j_trump_r.pdf   \n",
       "21  2019_donald_j_trump_r.pdf   \n",
       "22  2020_donald_j_trump_r.pdf   \n",
       "23  2020_donald_j_trump_r.pdf   \n",
       "24  2021_joseph_r_biden_d.pdf   \n",
       "25  2021_joseph_r_biden_d.pdf   \n",
       "\n",
       "                                             Sentence  \n",
       "0   Tax Code by finally ending the tax breaks for ...  \n",
       "1   money to pad their paychecks or buy fancy drap...  \n",
       "2   me his allowance and asked if I would give it ...  \n",
       "3             That's what helped us into this crisis.  \n",
       "4   you want to make a difference in the life of a...  \n",
       "5   into a system that's not working, we launched ...  \n",
       "6   this Congress needs to stop the interest rates...  \n",
       "7   have earned, which is why we've increased annu...  \n",
       "8   This single step would raise the incomes of mi...  \n",
       "9                            But we can't stop there.  \n",
       "10  A few months later, on his 10th deployment, Co...  \n",
       "11  have declared that China is no longer the worl...  \n",
       "12  every day, live the idea that we are our broth...  \n",
       "13  Every three weeks, we bring online as much sol...  \n",
       "14  We do them no favor when we don't show  \\nthem...  \n",
       "15  But most of all, it contradicts everything tha...  \n",
       "16     It's been a long time since we had fair trade.  \n",
       "17             We are  \\none people with one destiny.  \n",
       "18   And I asked CJ, \"What's the secret?\"  \\nHe sa...  \n",
       "19  And it's the people who  \\nare making America ...  \n",
       "20  to prohibit the late-term abortion of children...  \n",
       "21  In 1997, Alice was sentenced to life in prison...  \n",
       "22  The days of our country being used, taken adva...  \n",
       "23  because no parent should be forced to send the...  \n",
       "24  file:///C/WalidLenovo/SEAS6800Local/USBankLLM/...  \n",
       "25  history has been one of the greatest logistica...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "\n",
    "RND_SENTENCES = 2\n",
    "SENT_WORD_LENGTH = 20\n",
    "\n",
    "\n",
    "def extract_sentences_from_pdf(file_path, rnd_num_sentences=RND_SENTENCES):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "\n",
    "        # Split the text into sentences based on full stops\n",
    "        sentences = re.split(\n",
    "            r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "        # Randomly select 'rnd_num_sentences' sentences\n",
    "        if len(sentences) >= rnd_num_sentences:\n",
    "            selected_sentences = random.sample(sentences, rnd_num_sentences)\n",
    "        else:\n",
    "            selected_sentences = sentences\n",
    "\n",
    "        return selected_sentences\n",
    "\n",
    "\n",
    "# pdf_files = ['../data/2009_barack_obama_d.pdf', '../data/2010_barack_obama_d.pdf', '../data/2021_joseph_r_biden_d.pdf', '../data/sample_nda.pdf']\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    sentences = extract_sentences_from_pdf(file_path)\n",
    "    for sentence in sentences:\n",
    "        # Extract the last part of the sentence (around 10-15 words)\n",
    "        words = sentence.split()\n",
    "        if len(words) > SENT_WORD_LENGTH:\n",
    "            sentence = \" \".join(words[-15:])\n",
    "\n",
    "        file_name = os.path.basename(file_path)\n",
    "        all_data.append((file_name, sentence))\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=[\"File Name\", \"Sentence\"])\n",
    "df.to_csv(\"random_sentences_output.csv\", index=False)\n",
    "df  # This will display the DataFrame in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the code the guys gave does not work as is...at least for me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralDB' object has no attribute 'PDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\WalidLenovo\\SEAS6800Local\\USBankLLM\\notebooks\\ThirdAI.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#pdf_files = ['../data/sample_nda.pdf']\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m pdf_files:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pdf_doc \u001b[39m=\u001b[39m ndb\u001b[39m.\u001b[39;49mPDF(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     insertable_docs\u001b[39m.\u001b[39mappend(pdf_files)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# my_dict = {str(i): item for i, item in enumerate(insertable_docs)}\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WalidLenovo/SEAS6800Local/USBankLLM/notebooks/ThirdAI.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Result: {0: 'a', 1: 'b', 2: 'c'}\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralDB' object has no attribute 'PDF'"
     ]
    }
   ],
   "source": [
    "insertable_docs = []\n",
    "# pdf_files = ['../data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = ndb.PDF(file)\n",
    "\n",
    "    insertable_docs.append(pdf_files)\n",
    "\n",
    "    # my_dict = {str(i): item for i, item in enumerate(insertable_docs)}\n",
    "    # Result: {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "source_ids = ndb.insert(insertable_docs, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wnhas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code inserts one file at a time from a predefined directory (../daa...etc) and saves insertion\n",
    "\n",
    "#### times into a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting File: ../data/2009_barack_obama_d.pdf\n",
      "time taken to insert file 21.980719000042882\n",
      "\n",
      "Inserting File: ../data/2010_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 10.88914600003045\n",
      "\n",
      "Inserting File: ../data/2011_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 12.087964200007264\n",
      "\n",
      "Inserting File: ../data/2012_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 13.351022399961948\n",
      "\n",
      "Inserting File: ../data/2013_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 12.77896690001944\n",
      "\n",
      "Inserting File: ../data/2014_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 13.18003749998752\n",
      "\n",
      "Inserting File: ../data/2015_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 12.806928000005428\n",
      "\n",
      "Inserting File: ../data/2016_barack_obama_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 11.131390600057784\n",
      "\n",
      "Inserting File: ../data/2017_donald_j_trump_r.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 10.42964489996666\n",
      "\n",
      "Inserting File: ../data/2018_donald_j_trump_r.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 13.761067199986428\n",
      "\n",
      "Inserting File: ../data/2019_donald_j_trump_r.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 12.615262800012715\n",
      "\n",
      "Inserting File: ../data/2020_donald_j_trump_r.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 13.103856100002304\n",
      "\n",
      "Inserting File: ../data/2021_joseph_r_biden_d.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to insert file 17.498290000017732\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from thirdai.neural_db.documents import PDF\n",
    "from thirdai.neural_db.neural_db import NeuralDB\n",
    "\n",
    "# Create a dataframe to store the filename and duration of insertion for reference\n",
    "df_insertion_dur = pd.DataFrame(columns=['FileName', 'Duration'])\n",
    "# accompanying list for the durations\n",
    "durations_to_insert = []  # A list to hold all duration records as dictionaries\n",
    "\n",
    "# Create a NeuralDB instance\n",
    "ndb = NeuralDB()\n",
    "\n",
    "insertable_docs = []\n",
    "# pdf_files = ['../data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = PDF(file)  # Using PDF class directly\n",
    "    print(\"\\nInserting File:\", file)\n",
    "    start_time = time.perf_counter()\n",
    "    insertable_docs.append(\n",
    "        pdf_doc\n",
    "    )  # You should append the pdf_doc object, not pdf_files\n",
    "    # the insert function demands an interatable object hence the need to use a list and then clear it.\n",
    "    source_ids = ndb.insert(insertable_docs, train=True)\n",
    "    end_time = time.perf_counter()\n",
    "    insertable_docs.clear()\n",
    "    duration = end_time-start_time\n",
    "    durations_to_insert.append({'FileName': file, 'Duration': duration})\n",
    "    print(\"time taken to insert file\", duration)\n",
    "\n",
    "# save the durations data frame for further analysis.\n",
    "df_insertion_dur = pd.DataFrame(durations_to_insert)\n",
    "df_insertion_dur.to_csv(f\"{DIRECTORY_PATH}durations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below code is for searching. will modify it shortly to take the random sample sentences from the files and see if ThirdAI can pick them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will ensure equal treatment for all servicemembers and equal benefits for their families gay and straight. We will draw upon the courage and skills of our sisters and daughters and moms because women have proven under fire that they are ready for combat. We will keep faith with our veterans investing in world-class care-including mental health care-for our wounded warriors supporting our military families giving our veterans the benefits and education and job opportunities that they have earned.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "Can you blame them for feeling a little cynical? The greatest blow to our confidence in our economy last year didn't come from events beyond our control. It came from a debate in Washington over whether the United States would pay its bills or not. Who benefited from that fiasco?\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2012_barack_obama_d.pdf\n",
      "************\n",
      "So in the months ahead I will continue to engage Congress to ensure not only that our targeting detention and prosecution of terrorists remains consistent with our laws and system of checks and balances but that our efforts are even more transparent to the American people and to the world.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "It's a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet in America we do not enforce this rule straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences our current immigration system costs American taxpayers many billions of dollars a year.Switching away from this current system of lower skilled immigration and instead adopting a merit-based system we will have so many more benefits.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2017_donald_j_trump_r.pdf\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = ndb.search(\n",
    "    query=\"military benefits\",\n",
    "    top_k=4,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"),\n",
    ")\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print(\"************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code takes an excel sheet as input with a search sentence, and returns the 4\n",
    "\n",
    "#### sentences that it found with the source file info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:48: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'In other words we have lived through an era where too often short-term gains were prized over long-term prosperity where we failed to look beyond the next payment the next quarter or the next election. A surplus became an excuse to transfer wealth to the wealthy instead of an opportunity to invest in our future.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found Sentence_1\"] = found_sentence[0]\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2009_barack_obama_d.pdf' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found In File_1\"] = os.path.basename(found_file[0])\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'They're still the right thing to do. And I won't let up until they get done. But for my final address to this Chamber I don't want to just talk about next year. I want to focus on the next 5 years the next 10 years and beyond.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found Sentence_2\"] = found_sentence[1]\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:51: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2016_barack_obama_d.pdf' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found In File_2\"] = os.path.basename(found_file[1])\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'The regime is more isolated than ever before. Its leaders are faced with crippling sanctions and as long as they shirk their responsibilities this pressure will not relent. Let there be no doubt: America is determined to prevent Iran from getting a nuclear weapon and I will take no options off the table to achieve that goal.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found Sentence_3\"] = found_sentence[2]\n",
      "C:\\Users\\wnhas\\AppData\\Local\\Temp\\ipykernel_28284\\3510717697.py:53: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2012_barack_obama_d.pdf' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, \"Found In File_3\"] = os.path.basename(found_file[2])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentences from the CSV\n",
    "df = pd.read_csv(\"random_sentences_output.csv\")\n",
    "\n",
    "# query the neural DB and return the top 4 results with the associated files.\n",
    "\n",
    "\n",
    "def query_neural_db(sentence):\n",
    "    search_results = ndb.search(\n",
    "        query=sentence, top_k=4, on_error=lambda error_msg: print(f\"Error! {error_msg}\")\n",
    "    )\n",
    "\n",
    "    found_sentences = []\n",
    "    found_files = []\n",
    "\n",
    "    for result in search_results:\n",
    "        found_sentences.append(result.text)\n",
    "        found_files.append(result.source)\n",
    "\n",
    "    return found_sentences, found_files\n",
    "\n",
    "\n",
    "# Query the neural DB for each sentence\n",
    "for idx, row in df.iterrows():\n",
    "    search_string = row[\"Sentence\"]\n",
    "    found_sentence, found_file = query_neural_db(search_string)\n",
    "    # save the first 3 top results from the returned data from NeuralDB\n",
    "    df.at[idx, \"Found Sentence_1\"] = found_sentence[0]\n",
    "    df.at[idx, \"Found In File_1\"] = os.path.basename(found_file[0])\n",
    "    df.at[idx, \"Found Sentence_2\"] = found_sentence[1]\n",
    "    df.at[idx, \"Found In File_2\"] = os.path.basename(found_file[1])\n",
    "    df.at[idx, \"Found Sentence_3\"] = found_sentence[2]\n",
    "    df.at[idx, \"Found In File_3\"] = os.path.basename(found_file[2])\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"sentences_search.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\myusb_env\\Lib\\site-packages\\thirdai\\neural_db\\documents.py:552: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state[\"df\"] = state[\"df\"].applymap(path_to_str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\stored_dbs\\\\sample_state_of_union.ndb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save your db\n",
    "ndb.save(\"../stored_dbs/sample_state_of_union.ndb\")\n",
    "\n",
    "# <!-- # Loading is just like we showed above, with an optional progress handler\n",
    "# db.from_checkpoint(\"sample_nda.ndb\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\")) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.666666666666664% done with loading.\n",
      "33.33333333333333% done with loading.\n",
      "50.0% done with loading.\n",
      "66.66666666666666% done with loading.\n",
      "83.33333333333334% done with loading.\n",
      "100.0% done with loading.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<thirdai.neural_db.neural_db.NeuralDB at 0x214041c7cd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndb.from_checkpoint(\n",
    "    \"../stored_dbs/sample_state_of_union.ndb\",\n",
    "    on_progress=lambda fraction: print(f\"{fraction*100}% done with loading.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below is a retrieval of the stored data base and search for the same sentence\n",
    "\n",
    "#### intresting enough, not the same result!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will ensure equal treatment for all servicemembers and equal benefits for their families gay and straight. We will draw upon the courage and skills of our sisters and daughters and moms because women have proven under fire that they are ready for combat. We will keep faith with our veterans investing in world-class care-including mental health care-for our wounded warriors supporting our military families giving our veterans the benefits and education and job opportunities that they have earned.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "Can you blame them for feeling a little cynical? The greatest blow to our confidence in our economy last year didn't come from events beyond our control. It came from a debate in Washington over whether the United States would pay its bills or not. Who benefited from that fiasco?\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2012_barack_obama_d.pdf\n",
      "************\n",
      "So in the months ahead I will continue to engage Congress to ensure not only that our targeting detention and prosecution of terrorists remains consistent with our laws and system of checks and balances but that our efforts are even more transparent to the American people and to the world.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2013_barack_obama_d.pdf\n",
      "************\n",
      "It's a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet in America we do not enforce this rule straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences our current immigration system costs American taxpayers many billions of dollars a year.Switching away from this current system of lower skilled immigration and instead adopting a merit-based system we will have so many more benefits.\n",
      "c:\\WalidLenovo\\SEAS6800MNB\\USBLLM\\notebooks\\..\\data\\2017_donald_j_trump_r.pdf\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = ndb.search(\n",
    "    query=\"military benefits\",\n",
    "    top_k=4,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"),\n",
    ")\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print(\"************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
