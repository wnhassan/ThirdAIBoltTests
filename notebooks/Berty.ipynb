{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "input_text = \"Hello, BERT!\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "# This gives you the embeddings after all transformations.\n",
    "# If you specifically want just the initial embeddings:\n",
    "initial_embeddings = model.embeddings(inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "           3.8253e-02,  1.6400e-01],\n",
      "         [ 3.7386e-01, -1.5575e-02, -2.4561e-01,  ..., -3.1657e-02,\n",
      "           5.5144e-01, -5.2406e-01],\n",
      "         [ 4.6705e-04,  1.6225e-01, -6.4443e-02,  ...,  4.9443e-01,\n",
      "           6.9413e-01,  3.6286e-01],\n",
      "         [ 7.4566e-01,  2.8742e-01,  4.1331e-01,  ...,  8.7860e-01,\n",
      "           3.9919e-01, -2.0667e-01],\n",
      "         [ 6.4243e-01, -4.2258e-01, -4.0628e-01,  ...,  6.2612e-01,\n",
      "           5.6107e-01,  5.0588e-01],\n",
      "         [-3.2507e-01, -3.1879e-01, -1.1632e-01,  ..., -3.9602e-01,\n",
      "           4.1120e-01, -7.7552e-02]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "print (initial_embeddings)\n",
    "print (type(initial_embeddings))\n",
    "print(initial_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This shows that the Bank word has different context in its embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the embeddings: 0.5257286429405212\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to get the embedding of a word from a sentence\n",
    "def get_word_embedding(sentence, word):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "    word_position = inputs[\"input_ids\"][0].tolist().index(word_id)\n",
    "    return outputs[\"last_hidden_state\"][0][word_position].detach().numpy()\n",
    "\n",
    "# Compare embeddings for the word 'bank' in two different contexts\n",
    "sentence1 = \"I sat by the river bank.\"\n",
    "sentence2 = \"I deposited money in the bank.\"\n",
    "\n",
    "embedding1 = get_word_embedding(sentence1, \"bank\")\n",
    "embedding2 = get_word_embedding(sentence2, \"bank\")\n",
    "\n",
    "# Calculate cosine similarity or any other metric to see the difference\n",
    "# For simplicity, let's use dot product\n",
    "similarity = torch.nn.functional.cosine_similarity(\n",
    "    torch.tensor(embedding1).unsqueeze(0), torch.tensor(embedding2).unsqueeze(0)\n",
    ")\n",
    "\n",
    "print(f\"Cosine similarity between the embeddings: {similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial value of the embeddings for the word bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the initial embeddings: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to get the initial embedding of a word from a sentence\n",
    "def get_initial_word_embedding(sentence, word):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "    word_position = inputs[\"input_ids\"][0].tolist().index(word_id)\n",
    "    \n",
    "    # Extracting the initial embeddings\n",
    "    initial_embeddings = model.embeddings(inputs[\"input_ids\"])\n",
    "    \n",
    "    return initial_embeddings[0][word_position].detach().numpy()\n",
    "\n",
    "# Compare initial embeddings for the word 'bank' in two different contexts\n",
    "sentence1 = \"I sat by the river bank.\"\n",
    "sentence2 = \"I deposited money in the bank.\"\n",
    "\n",
    "embedding1 = get_initial_word_embedding(sentence1, \"bank\")\n",
    "embedding2 = get_initial_word_embedding(sentence2, \"bank\")\n",
    "\n",
    "# Calculate cosine similarity or any other metric to see the difference\n",
    "# For simplicity, let's use dot product\n",
    "similarity = torch.nn.functional.cosine_similarity(\n",
    "    torch.tensor(embedding1).unsqueeze(0), torch.tensor(embedding2).unsqueeze(0)\n",
    ")\n",
    "\n",
    "print(f\"Cosine similarity between the initial embeddings: {similarity.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us track the similarity starting to drift thru the layers as attentions and context gets changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 (initial embeddings) similarity: 0.9999998807907104\n",
      "Layer 1 similarity: 0.7583111524581909\n",
      "Layer 2 similarity: 0.6887192726135254\n",
      "Layer 3 similarity: 0.6551121473312378\n",
      "Layer 4 similarity: 0.5860087275505066\n",
      "Layer 5 similarity: 0.5718533396720886\n",
      "Layer 6 similarity: 0.5652674436569214\n",
      "Layer 7 similarity: 0.5206235647201538\n",
      "Layer 8 similarity: 0.49700790643692017\n",
      "Layer 9 similarity: 0.4941667318344116\n",
      "Layer 10 similarity: 0.5007324814796448\n",
      "Layer 11 similarity: 0.5522936582565308\n",
      "Layer 12 similarity: 0.5257290005683899\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the sentences\n",
    "sentence1 = \"I sat by the river bank.\"\n",
    "sentence2 = \"I deposited money in the bank.\"\n",
    "\n",
    "inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "word_id = tokenizer.convert_tokens_to_ids(\"bank\")\n",
    "word_position1 = inputs1[\"input_ids\"][0].tolist().index(word_id)\n",
    "word_position2 = inputs2[\"input_ids\"][0].tolist().index(word_id)\n",
    "\n",
    "# Get initial embeddings\n",
    "initial_embeddings1 = model.embeddings(inputs1[\"input_ids\"])\n",
    "initial_embeddings2 = model.embeddings(inputs2[\"input_ids\"])\n",
    "\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(initial_embeddings1[0][word_position1].unsqueeze(0),\n",
    "                                                   initial_embeddings2[0][word_position2].unsqueeze(0))\n",
    "print(f\"Layer 0 (initial embeddings) similarity: {cosine_sim.item()}\")\n",
    "\n",
    "# Process both sentences through each BERT layer\n",
    "hidden_states1 = [initial_embeddings1]\n",
    "hidden_states2 = [initial_embeddings2]\n",
    "\n",
    "for i, layer in enumerate(model.encoder.layer):\n",
    "    layer_output1 = layer(hidden_states1[-1], attention_mask=inputs1[\"attention_mask\"])\n",
    "    hidden_states1.append(layer_output1[0])\n",
    "    \n",
    "    layer_output2 = layer(hidden_states2[-1], attention_mask=inputs2[\"attention_mask\"])\n",
    "    hidden_states2.append(layer_output2[0])\n",
    "    \n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(hidden_states1[-1][0][word_position1].unsqueeze(0),\n",
    "                                                       hidden_states2[-1][0][word_position2].unsqueeze(0))\n",
    "    print(f\"Layer {i + 1} similarity: {cosine_sim.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myusb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
